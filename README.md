# Transfer-learning-SOLAR-ENERGY-MATERIALS

We build a 3-layer network with dropout rate 0.2 for the first 2 layers. We first trained the model with the pretraindataset, using MSELoss as our criterion and Adam as our optimiser. Then we freeze the the parameters and train the model with training data for 2000 epochs. Afterwards, we pause training, unfreeze the body, and continue training, but with a very small learning rate for 2000 epochs.

Discover promising materials for organic solar cells by predicting the HOMO-LUMO gap from a molecule description. Traditionally, density functional theory is used to estimate properties of molecules. However, this is computationally very expensive. Instead, we would like to use machine learning to predict the HOMO-LUMO gap from the molecule description directly. Using the machine learning model you will develop, we can efficiently sift through millions of potential molecule configurations and select the most promising ones for downstream experiments.

There is one catch: we are only given a very limited number of 100 labeled data points, i.e., 100 (molecule, HOMO-LUMO gap) tuples. However, there is an additional data set available that you can be used to improve our accuracy of predicting the HOMO-LUMO gap. Specifically, we have another related data set of 50,000 labeled (molecule, ELUMO) pairs to supplement the 100 data points for solving our objective. Note that in the second, larger data set, the labels (i.e. the “y-values”) are not the quantity we are interested in (i.e., not the HOMO-LUMO gap), but instead another, highly related quantity: the LUMO energy level ELUMO alone (cf. Table 1). The molecules in both data sets are not overlapping.

The key insight is that the ability of predicting the LUMO energy given a molecule description can transfer to the ability of predicting the HOMO-LUMO gap. In particular, molecule features that are predictive of the LUMO energy are likely also predictive of the HOMO energy and therefore also the HOMO-LUMO gap. In other words, being given a related data set, we should not start “from scratch” when solving the prediction task of the HOMO-LUMO gap from only 100 examples. Instead, the features learned on one task should transfer to the other task, and thus the larger data set of (molecule, ELUMO) pairs should be used to improve prediction of the HOMO-LUMO gap. In machine learning parlance, this is called transfer learning. This setting is very realistic in many practical applications, where we rarely have a lot of data for the exact task we are interested in.

Unsupervised representation learning, using an autoencoder. Similar ideas are used to extract useful feature representations for molecules from the large dataset to improve the accuracy of predicting the HOMO-LUMO gap. 
The difference: should “transfer” your knowledge to using the additional label information in the large data set. For instance, analogous to the encoder of an autoencoder, re-use the feature representation of the last layer when training a neural network model to predict the LUMO energy. This is common practice in, for example, computer vision. Given the difference in sizes of the two available data sets, your performance on the test set likely depends much more on how you make use of the large, pretraining data set versus the small, training data set. 
