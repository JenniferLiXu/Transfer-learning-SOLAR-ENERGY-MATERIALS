{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4bb09e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "621a571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "x_pretrain = pd.read_csv('pretrain_features.csv').drop(columns=['Id', 'smiles']).values\n",
    "y_pretrain = pd.read_csv('pretrain_labels.csv').drop(columns='Id').values\n",
    "\n",
    "#x_test = pd.read_csv('train_features.csv').drop(columns=['Id', 'smiles']).values\n",
    "#y_test = pd.read_csv('train_labels.csv').drop(columns='Id').values\n",
    "\n",
    "\n",
    "X_pre, X_prevalid, y_pre, y_prevalid = train_test_split(x_pretrain, y_pretrain, test_size= 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adcdc6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.from_numpy(x).float()\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "        self.len = x.shape[0]\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return (self.x[key],self.y[key])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d38f9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.5],std=[0.5])])\n",
    "\n",
    "\n",
    "myDataset_pretrain = MyDataset(X_pre, y_pre)\n",
    "dataLoader_pretrain = DataLoader(dataset=myDataset_pretrain, batch_size=128, shuffle=True)\n",
    "\n",
    "myDataset_test = MyDataset(X_prevalid, y_prevalid)\n",
    "dataLoader_test = DataLoader(dataset=myDataset_test, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19af35c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layer1): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # create Sequential objects\n",
    "        self.layer1 = nn.Sequential(nn.Linear(1000, 64), nn.ReLU(), nn.Dropout(0.25))\n",
    "        #self.layer2 = nn.Sequential(nn.Linear(128, 32), nn.ReLU(), nn.Dropout(0.2))\n",
    "        self.layer3 = nn.Sequential(nn.Linear(64, 1))\n",
    "        self.layers = [attr for attr in dir(self) if 'layer' in attr]\n",
    "        self.layers.sort()\n",
    "\n",
    "    def forward(self, x):\n",
    "        layers = [getattr(self, layer) for layer in self.layers]\n",
    "        for layer in layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    # def get_layer_2(self, x):\n",
    "    #     x = self.layer1(x)\n",
    "    #     x = self.layer2(x)\n",
    "    #     # x = self.layer3(x)\n",
    "    #     return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe5c18ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define learning rate\n",
    "lr = 3e-5\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "# adam梯 \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75236738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best checkpoint from epoch 0.\n",
      "epoch: 0, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "Saving best checkpoint from epoch 1.\n",
      "epoch: 1, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "epoch: 2, Train Loss: 0.000049,Eval Loss: 0.000036\n",
      "Saving best checkpoint from epoch 3.\n",
      "epoch: 3, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 4, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 5, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "epoch: 6, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "Saving best checkpoint from epoch 7.\n",
      "epoch: 7, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 8, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "epoch: 9, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "epoch: 10, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "epoch: 11, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "epoch: 12, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "Saving best checkpoint from epoch 13.\n",
      "epoch: 13, Train Loss: 0.000049,Eval Loss: 0.000035\n",
      "epoch: 14, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 15, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 16, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "epoch: 17, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "epoch: 18, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "epoch: 19, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "epoch: 20, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "epoch: 21, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "epoch: 22, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "epoch: 23, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 24, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 25, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "Saving best checkpoint from epoch 26.\n",
      "epoch: 26, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 27, Train Loss: 0.000049,Eval Loss: 0.000035\n",
      "epoch: 28, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "Saving best checkpoint from epoch 29.\n",
      "epoch: 29, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 30, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 31, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "epoch: 32, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "epoch: 33, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 34, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "epoch: 35, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 36, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "epoch: 37, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 38, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 39, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "epoch: 40, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 41, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 42, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "epoch: 43, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "epoch: 44, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 45, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 46, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "epoch: 47, Train Loss: 0.000049,Eval Loss: 0.000035\n",
      "Saving best checkpoint from epoch 48.\n",
      "epoch: 48, Train Loss: 0.000047,Eval Loss: 0.000035\n",
      "Saving best checkpoint from epoch 49.\n",
      "epoch: 49, Train Loss: 0.000047,Eval Loss: 0.000035\n",
      "epoch: 50, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "epoch: 51, Train Loss: 0.000047,Eval Loss: 0.000035\n",
      "epoch: 52, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "epoch: 53, Train Loss: 0.000049,Eval Loss: 0.000035\n",
      "epoch: 54, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 55, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "epoch: 56, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 57, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 58, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 59, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 60, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 61, Train Loss: 0.000047,Eval Loss: 0.000035\n",
      "epoch: 62, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "epoch: 63, Train Loss: 0.000047,Eval Loss: 0.000035\n",
      "epoch: 64, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "epoch: 65, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 66, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "epoch: 67, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 68, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "Saving best checkpoint from epoch 69.\n",
      "epoch: 69, Train Loss: 0.000047,Eval Loss: 0.000035\n",
      "epoch: 70, Train Loss: 0.000047,Eval Loss: 0.000035\n",
      "epoch: 71, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "epoch: 72, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "epoch: 73, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 74, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 75, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 76, Train Loss: 0.000049,Eval Loss: 0.000035\n",
      "epoch: 77, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "epoch: 78, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 79, Train Loss: 0.000049,Eval Loss: 0.000036\n",
      "epoch: 80, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "Saving best checkpoint from epoch 81.\n",
      "epoch: 81, Train Loss: 0.000047,Eval Loss: 0.000035\n",
      "Saving best checkpoint from epoch 82.\n",
      "epoch: 82, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "Saving best checkpoint from epoch 83.\n",
      "epoch: 83, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 84, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 85, Train Loss: 0.000047,Eval Loss: 0.000035\n",
      "epoch: 86, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 87, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 88, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 89, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "Saving best checkpoint from epoch 90.\n",
      "epoch: 90, Train Loss: 0.000047,Eval Loss: 0.000035\n",
      "epoch: 91, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 92, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 93, Train Loss: 0.000047,Eval Loss: 0.000035\n",
      "epoch: 94, Train Loss: 0.000047,Eval Loss: 0.000035\n",
      "epoch: 95, Train Loss: 0.000048,Eval Loss: 0.000035\n",
      "epoch: 96, Train Loss: 0.000047,Eval Loss: 0.000035\n",
      "epoch: 97, Train Loss: 0.000048,Eval Loss: 0.000036\n",
      "epoch: 98, Train Loss: 0.000047,Eval Loss: 0.000036\n",
      "epoch: 99, Train Loss: 0.000048,Eval Loss: 0.000036\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_losses = []\n",
    "train_acces = []\n",
    "\n",
    "eval_losses = []\n",
    "eval_acces = []\n",
    "\n",
    "min_eval_loss = 1e5\n",
    "\n",
    "# train the network\n",
    "for e in range(2000):\n",
    "\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    net.train()   #\n",
    "\n",
    "    for features, labels in dataLoader_pretrain:\n",
    "\n",
    "        # forward, get loss \n",
    "        outputs = net(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backpropagation，set the previous gradient to zero，use step function to update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # record the loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_losses.append(train_loss / myDataset_pretrain.len)\n",
    "\n",
    "    \n",
    "    # test on validation set\n",
    "    eval_loss = 0\n",
    "    eval_acc = 0\n",
    "    net.eval()  \n",
    "\n",
    "    # updated each time by a small group of data \n",
    "    for features, labels in dataLoader_test:\n",
    "        features = Variable(features)  \n",
    "        labels = Variable(labels)  \n",
    "    \n",
    "\n",
    "        outputs = net(features)  \n",
    "        # label = label.unsqueeze(1)\n",
    "        loss = criterion(outputs, labels)  \n",
    "\n",
    "        #record the loss\n",
    "        eval_loss += loss.item()\n",
    "        \n",
    "#         pred = outputs.max(1)[1]\n",
    "#         num_correct = (pred==labels).sum().item()\n",
    "#         acc = num_correct / x.shape[0]\n",
    "#         eval_acc += acc\n",
    "    if eval_loss < min_eval_loss:\n",
    "        print(f\"Saving best checkpoint from epoch {e}.\")\n",
    "        torch.save(net.state_dict(), \"task_4_best_pretrained_model.pth\")\n",
    "        min_eval_loss = eval_loss\n",
    "\n",
    "    eval_losses.append(eval_loss / myDataset_test.len)\n",
    "    # eval_acces.append(eval_acc / myDataset_test.len)\n",
    "    print('epoch: {}, Train Loss: {:.6f},Eval Loss: {:.6f}'\n",
    "           .format(e, train_loss / myDataset_pretrain.len, eval_loss / myDataset_test.len))\n",
    "    #log_writer.add_scalar(\"loss/train\", float(train_loss), e)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa73875c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('train_features.csv').drop(columns=['Id', 'smiles']).values\n",
    "y_train = pd.read_csv('train_labels.csv').drop(columns='Id').values\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size= 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db22fb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDataset_train = MyDataset(X_train, y_train)\n",
    "dataLoader_train = DataLoader(dataset=myDataset_train, batch_size=4, shuffle=True)\n",
    "\n",
    "myDataset_valid = MyDataset(X_valid, y_valid)\n",
    "dataLoader_valid = DataLoader(dataset=myDataset_valid, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d12a1b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    net_copy = copy.deepcopy(net_copy)\n",
    "except:\n",
    "    net_copy = copy.deepcopy(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8130fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_net = Net()\n",
    "finetune_net.load_state_dict(torch.load(\"task_4_best_pretrained_model.pth\"))\n",
    "for param in finetune_net.parameters():\n",
    "    if param.shape[0]!=1:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8724a896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1000])\n",
      "torch.Size([64])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for param in finetune_net.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f80f221c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, Train Loss: 0.004638,Eval Loss: 0.073662\n",
      "epoch: 1, Train Loss: 0.004172,Eval Loss: 0.073637\n",
      "epoch: 2, Train Loss: 0.004245,Eval Loss: 0.073656\n",
      "epoch: 3, Train Loss: 0.004286,Eval Loss: 0.073643\n",
      "epoch: 4, Train Loss: 0.005515,Eval Loss: 0.073621\n",
      "epoch: 5, Train Loss: 0.005675,Eval Loss: 0.073613\n",
      "epoch: 6, Train Loss: 0.003697,Eval Loss: 0.073615\n",
      "epoch: 7, Train Loss: 0.003401,Eval Loss: 0.073620\n",
      "epoch: 8, Train Loss: 0.005081,Eval Loss: 0.073623\n",
      "epoch: 9, Train Loss: 0.004678,Eval Loss: 0.073587\n",
      "epoch: 10, Train Loss: 0.005962,Eval Loss: 0.073600\n",
      "epoch: 11, Train Loss: 0.004030,Eval Loss: 0.073600\n",
      "epoch: 12, Train Loss: 0.006070,Eval Loss: 0.073605\n",
      "epoch: 13, Train Loss: 0.003070,Eval Loss: 0.073575\n",
      "epoch: 14, Train Loss: 0.005284,Eval Loss: 0.073588\n",
      "epoch: 15, Train Loss: 0.003989,Eval Loss: 0.073583\n",
      "epoch: 16, Train Loss: 0.003438,Eval Loss: 0.073571\n",
      "epoch: 17, Train Loss: 0.004354,Eval Loss: 0.073605\n",
      "epoch: 18, Train Loss: 0.004589,Eval Loss: 0.073638\n",
      "epoch: 19, Train Loss: 0.003406,Eval Loss: 0.073643\n",
      "epoch: 20, Train Loss: 0.003686,Eval Loss: 0.073637\n",
      "epoch: 21, Train Loss: 0.004548,Eval Loss: 0.073644\n",
      "epoch: 22, Train Loss: 0.003920,Eval Loss: 0.073673\n",
      "epoch: 23, Train Loss: 0.002810,Eval Loss: 0.073706\n",
      "epoch: 24, Train Loss: 0.005484,Eval Loss: 0.073705\n",
      "epoch: 25, Train Loss: 0.003512,Eval Loss: 0.073666\n",
      "epoch: 26, Train Loss: 0.004398,Eval Loss: 0.073678\n",
      "epoch: 27, Train Loss: 0.004759,Eval Loss: 0.073643\n",
      "epoch: 28, Train Loss: 0.004639,Eval Loss: 0.073668\n",
      "epoch: 29, Train Loss: 0.003977,Eval Loss: 0.073698\n",
      "epoch: 30, Train Loss: 0.004335,Eval Loss: 0.073684\n",
      "epoch: 31, Train Loss: 0.003190,Eval Loss: 0.073742\n",
      "epoch: 32, Train Loss: 0.005307,Eval Loss: 0.073760\n",
      "epoch: 33, Train Loss: 0.003742,Eval Loss: 0.073746\n",
      "epoch: 34, Train Loss: 0.003988,Eval Loss: 0.073752\n",
      "epoch: 35, Train Loss: 0.005401,Eval Loss: 0.073824\n",
      "epoch: 36, Train Loss: 0.003563,Eval Loss: 0.073843\n",
      "epoch: 37, Train Loss: 0.003644,Eval Loss: 0.073891\n",
      "epoch: 38, Train Loss: 0.005346,Eval Loss: 0.073937\n",
      "epoch: 39, Train Loss: 0.004021,Eval Loss: 0.073957\n",
      "epoch: 40, Train Loss: 0.005489,Eval Loss: 0.073999\n",
      "epoch: 41, Train Loss: 0.003943,Eval Loss: 0.073994\n",
      "epoch: 42, Train Loss: 0.004253,Eval Loss: 0.073957\n",
      "epoch: 43, Train Loss: 0.005367,Eval Loss: 0.073971\n",
      "epoch: 44, Train Loss: 0.003890,Eval Loss: 0.073971\n",
      "epoch: 45, Train Loss: 0.004322,Eval Loss: 0.074005\n",
      "epoch: 46, Train Loss: 0.003829,Eval Loss: 0.074016\n",
      "epoch: 47, Train Loss: 0.004527,Eval Loss: 0.074030\n",
      "epoch: 48, Train Loss: 0.003950,Eval Loss: 0.073984\n",
      "epoch: 49, Train Loss: 0.004567,Eval Loss: 0.073905\n",
      "epoch: 50, Train Loss: 0.004210,Eval Loss: 0.073908\n",
      "epoch: 51, Train Loss: 0.004066,Eval Loss: 0.073915\n",
      "epoch: 52, Train Loss: 0.003943,Eval Loss: 0.073883\n",
      "epoch: 53, Train Loss: 0.003943,Eval Loss: 0.073945\n",
      "epoch: 54, Train Loss: 0.004225,Eval Loss: 0.073958\n",
      "epoch: 55, Train Loss: 0.004445,Eval Loss: 0.073895\n",
      "epoch: 56, Train Loss: 0.003589,Eval Loss: 0.073871\n",
      "epoch: 57, Train Loss: 0.003591,Eval Loss: 0.073854\n",
      "epoch: 58, Train Loss: 0.003698,Eval Loss: 0.073803\n",
      "epoch: 59, Train Loss: 0.006103,Eval Loss: 0.073910\n",
      "epoch: 60, Train Loss: 0.005160,Eval Loss: 0.073935\n",
      "epoch: 61, Train Loss: 0.004411,Eval Loss: 0.073973\n",
      "epoch: 62, Train Loss: 0.003911,Eval Loss: 0.073975\n",
      "epoch: 63, Train Loss: 0.005298,Eval Loss: 0.074022\n",
      "epoch: 64, Train Loss: 0.003952,Eval Loss: 0.074076\n",
      "epoch: 65, Train Loss: 0.004135,Eval Loss: 0.074120\n",
      "epoch: 66, Train Loss: 0.004523,Eval Loss: 0.074135\n",
      "epoch: 67, Train Loss: 0.003454,Eval Loss: 0.074167\n",
      "epoch: 68, Train Loss: 0.004450,Eval Loss: 0.074146\n",
      "epoch: 69, Train Loss: 0.005766,Eval Loss: 0.074083\n",
      "epoch: 70, Train Loss: 0.007294,Eval Loss: 0.074067\n",
      "epoch: 71, Train Loss: 0.004270,Eval Loss: 0.074087\n",
      "epoch: 72, Train Loss: 0.004215,Eval Loss: 0.074053\n",
      "epoch: 73, Train Loss: 0.003430,Eval Loss: 0.074007\n",
      "epoch: 74, Train Loss: 0.005745,Eval Loss: 0.074031\n",
      "epoch: 75, Train Loss: 0.003714,Eval Loss: 0.074010\n",
      "epoch: 76, Train Loss: 0.005941,Eval Loss: 0.073988\n",
      "epoch: 77, Train Loss: 0.004842,Eval Loss: 0.073972\n",
      "epoch: 78, Train Loss: 0.005264,Eval Loss: 0.073946\n",
      "epoch: 79, Train Loss: 0.004563,Eval Loss: 0.073978\n",
      "epoch: 80, Train Loss: 0.004033,Eval Loss: 0.073947\n",
      "epoch: 81, Train Loss: 0.005035,Eval Loss: 0.073897\n",
      "epoch: 82, Train Loss: 0.005097,Eval Loss: 0.073931\n",
      "epoch: 83, Train Loss: 0.003725,Eval Loss: 0.073929\n",
      "epoch: 84, Train Loss: 0.003878,Eval Loss: 0.073875\n",
      "epoch: 85, Train Loss: 0.004386,Eval Loss: 0.073884\n",
      "epoch: 86, Train Loss: 0.004303,Eval Loss: 0.073921\n",
      "epoch: 87, Train Loss: 0.004188,Eval Loss: 0.073981\n",
      "epoch: 88, Train Loss: 0.003550,Eval Loss: 0.073981\n",
      "epoch: 89, Train Loss: 0.004277,Eval Loss: 0.073949\n",
      "epoch: 90, Train Loss: 0.004211,Eval Loss: 0.073930\n",
      "epoch: 91, Train Loss: 0.003852,Eval Loss: 0.073894\n",
      "epoch: 92, Train Loss: 0.004793,Eval Loss: 0.073886\n",
      "epoch: 93, Train Loss: 0.004231,Eval Loss: 0.073878\n",
      "epoch: 94, Train Loss: 0.004298,Eval Loss: 0.073863\n",
      "epoch: 95, Train Loss: 0.004483,Eval Loss: 0.073828\n",
      "epoch: 96, Train Loss: 0.004089,Eval Loss: 0.073804\n",
      "epoch: 97, Train Loss: 0.005033,Eval Loss: 0.073820\n",
      "epoch: 98, Train Loss: 0.004191,Eval Loss: 0.073872\n",
      "epoch: 99, Train Loss: 0.003575,Eval Loss: 0.073849\n",
      "epoch: 100, Train Loss: 0.003799,Eval Loss: 0.073818\n",
      "epoch: 101, Train Loss: 0.005286,Eval Loss: 0.073806\n",
      "epoch: 102, Train Loss: 0.003349,Eval Loss: 0.073755\n",
      "epoch: 103, Train Loss: 0.005463,Eval Loss: 0.073707\n",
      "epoch: 104, Train Loss: 0.004460,Eval Loss: 0.073762\n",
      "epoch: 105, Train Loss: 0.004910,Eval Loss: 0.073780\n",
      "epoch: 106, Train Loss: 0.005898,Eval Loss: 0.073828\n",
      "epoch: 107, Train Loss: 0.004606,Eval Loss: 0.073849\n",
      "epoch: 108, Train Loss: 0.004147,Eval Loss: 0.073842\n",
      "epoch: 109, Train Loss: 0.004516,Eval Loss: 0.073805\n",
      "epoch: 110, Train Loss: 0.004938,Eval Loss: 0.073746\n",
      "epoch: 111, Train Loss: 0.004632,Eval Loss: 0.073710\n",
      "epoch: 112, Train Loss: 0.006641,Eval Loss: 0.073743\n",
      "epoch: 113, Train Loss: 0.004601,Eval Loss: 0.073702\n",
      "epoch: 114, Train Loss: 0.003862,Eval Loss: 0.073678\n",
      "epoch: 115, Train Loss: 0.003650,Eval Loss: 0.073689\n",
      "epoch: 116, Train Loss: 0.004876,Eval Loss: 0.073733\n",
      "epoch: 117, Train Loss: 0.003652,Eval Loss: 0.073711\n",
      "epoch: 118, Train Loss: 0.004031,Eval Loss: 0.073687\n",
      "epoch: 119, Train Loss: 0.004195,Eval Loss: 0.073630\n",
      "epoch: 120, Train Loss: 0.003882,Eval Loss: 0.073599\n",
      "epoch: 121, Train Loss: 0.004428,Eval Loss: 0.073567\n",
      "epoch: 122, Train Loss: 0.004991,Eval Loss: 0.073581\n",
      "epoch: 123, Train Loss: 0.003901,Eval Loss: 0.073594\n",
      "epoch: 124, Train Loss: 0.005583,Eval Loss: 0.073577\n",
      "epoch: 125, Train Loss: 0.004336,Eval Loss: 0.073587\n",
      "epoch: 126, Train Loss: 0.004458,Eval Loss: 0.073613\n",
      "epoch: 127, Train Loss: 0.004546,Eval Loss: 0.073677\n",
      "epoch: 128, Train Loss: 0.005161,Eval Loss: 0.073727\n",
      "epoch: 129, Train Loss: 0.004650,Eval Loss: 0.073700\n",
      "epoch: 130, Train Loss: 0.005374,Eval Loss: 0.073631\n",
      "epoch: 131, Train Loss: 0.004938,Eval Loss: 0.073660\n",
      "epoch: 132, Train Loss: 0.005175,Eval Loss: 0.073718\n",
      "epoch: 133, Train Loss: 0.005101,Eval Loss: 0.073750\n",
      "epoch: 134, Train Loss: 0.004181,Eval Loss: 0.073822\n",
      "epoch: 135, Train Loss: 0.003830,Eval Loss: 0.073845\n",
      "epoch: 136, Train Loss: 0.004971,Eval Loss: 0.073770\n",
      "epoch: 137, Train Loss: 0.004398,Eval Loss: 0.073778\n",
      "epoch: 138, Train Loss: 0.004586,Eval Loss: 0.073767\n",
      "epoch: 139, Train Loss: 0.004682,Eval Loss: 0.073752\n",
      "epoch: 140, Train Loss: 0.003893,Eval Loss: 0.073701\n",
      "epoch: 141, Train Loss: 0.003259,Eval Loss: 0.073718\n",
      "epoch: 142, Train Loss: 0.003712,Eval Loss: 0.073746\n",
      "epoch: 143, Train Loss: 0.005131,Eval Loss: 0.073709\n",
      "epoch: 144, Train Loss: 0.005058,Eval Loss: 0.073682\n",
      "epoch: 145, Train Loss: 0.004923,Eval Loss: 0.073651\n",
      "epoch: 146, Train Loss: 0.004095,Eval Loss: 0.073654\n",
      "epoch: 147, Train Loss: 0.004073,Eval Loss: 0.073660\n",
      "epoch: 148, Train Loss: 0.003855,Eval Loss: 0.073613\n",
      "epoch: 149, Train Loss: 0.004804,Eval Loss: 0.073605\n",
      "epoch: 150, Train Loss: 0.004084,Eval Loss: 0.073579\n",
      "epoch: 151, Train Loss: 0.003469,Eval Loss: 0.073567\n",
      "epoch: 152, Train Loss: 0.004476,Eval Loss: 0.073551\n",
      "epoch: 153, Train Loss: 0.004086,Eval Loss: 0.073530\n",
      "epoch: 154, Train Loss: 0.003932,Eval Loss: 0.073425\n",
      "epoch: 155, Train Loss: 0.003993,Eval Loss: 0.073429\n",
      "epoch: 156, Train Loss: 0.003682,Eval Loss: 0.073442\n",
      "epoch: 157, Train Loss: 0.004799,Eval Loss: 0.073451\n",
      "epoch: 158, Train Loss: 0.006290,Eval Loss: 0.073408\n",
      "epoch: 159, Train Loss: 0.004336,Eval Loss: 0.073369\n",
      "epoch: 160, Train Loss: 0.005230,Eval Loss: 0.073362\n",
      "epoch: 161, Train Loss: 0.004135,Eval Loss: 0.073353\n",
      "epoch: 162, Train Loss: 0.004237,Eval Loss: 0.073388\n",
      "epoch: 163, Train Loss: 0.004742,Eval Loss: 0.073367\n",
      "epoch: 164, Train Loss: 0.004489,Eval Loss: 0.073355\n",
      "epoch: 165, Train Loss: 0.004723,Eval Loss: 0.073403\n",
      "epoch: 166, Train Loss: 0.003973,Eval Loss: 0.073394\n",
      "epoch: 167, Train Loss: 0.004041,Eval Loss: 0.073401\n",
      "epoch: 168, Train Loss: 0.004324,Eval Loss: 0.073390\n",
      "epoch: 169, Train Loss: 0.003879,Eval Loss: 0.073351\n",
      "epoch: 170, Train Loss: 0.004537,Eval Loss: 0.073375\n",
      "epoch: 171, Train Loss: 0.004433,Eval Loss: 0.073414\n",
      "epoch: 172, Train Loss: 0.004088,Eval Loss: 0.073409\n",
      "epoch: 173, Train Loss: 0.004973,Eval Loss: 0.073463\n",
      "epoch: 174, Train Loss: 0.004544,Eval Loss: 0.073471\n",
      "epoch: 175, Train Loss: 0.004843,Eval Loss: 0.073451\n",
      "epoch: 176, Train Loss: 0.004599,Eval Loss: 0.073514\n",
      "epoch: 177, Train Loss: 0.004832,Eval Loss: 0.073548\n",
      "epoch: 178, Train Loss: 0.004494,Eval Loss: 0.073544\n",
      "epoch: 179, Train Loss: 0.003990,Eval Loss: 0.073608\n",
      "epoch: 180, Train Loss: 0.003631,Eval Loss: 0.073602\n",
      "epoch: 181, Train Loss: 0.004011,Eval Loss: 0.073579\n",
      "epoch: 182, Train Loss: 0.005001,Eval Loss: 0.073642\n",
      "epoch: 183, Train Loss: 0.003528,Eval Loss: 0.073642\n",
      "epoch: 184, Train Loss: 0.004201,Eval Loss: 0.073724\n",
      "epoch: 185, Train Loss: 0.003830,Eval Loss: 0.073767\n",
      "epoch: 186, Train Loss: 0.005280,Eval Loss: 0.073743\n",
      "epoch: 187, Train Loss: 0.003588,Eval Loss: 0.073746\n",
      "epoch: 188, Train Loss: 0.005610,Eval Loss: 0.073767\n",
      "epoch: 189, Train Loss: 0.005562,Eval Loss: 0.073755\n",
      "epoch: 190, Train Loss: 0.005568,Eval Loss: 0.073797\n",
      "epoch: 191, Train Loss: 0.004985,Eval Loss: 0.073733\n",
      "epoch: 192, Train Loss: 0.004598,Eval Loss: 0.073670\n",
      "epoch: 193, Train Loss: 0.006077,Eval Loss: 0.073702\n",
      "epoch: 194, Train Loss: 0.004623,Eval Loss: 0.073707\n",
      "epoch: 195, Train Loss: 0.004509,Eval Loss: 0.073680\n",
      "epoch: 196, Train Loss: 0.003302,Eval Loss: 0.073710\n",
      "epoch: 197, Train Loss: 0.003836,Eval Loss: 0.073668\n",
      "epoch: 198, Train Loss: 0.004403,Eval Loss: 0.073631\n",
      "epoch: 199, Train Loss: 0.004425,Eval Loss: 0.073615\n",
      "epoch: 200, Train Loss: 0.003870,Eval Loss: 0.073616\n",
      "epoch: 201, Train Loss: 0.003441,Eval Loss: 0.073599\n",
      "epoch: 202, Train Loss: 0.004364,Eval Loss: 0.073613\n",
      "epoch: 203, Train Loss: 0.004207,Eval Loss: 0.073686\n",
      "epoch: 204, Train Loss: 0.004226,Eval Loss: 0.073789\n",
      "epoch: 205, Train Loss: 0.004723,Eval Loss: 0.073817\n",
      "epoch: 206, Train Loss: 0.004145,Eval Loss: 0.073854\n",
      "epoch: 207, Train Loss: 0.005553,Eval Loss: 0.073858\n",
      "epoch: 208, Train Loss: 0.004008,Eval Loss: 0.073871\n",
      "epoch: 209, Train Loss: 0.004267,Eval Loss: 0.073806\n",
      "epoch: 210, Train Loss: 0.004383,Eval Loss: 0.073809\n",
      "epoch: 211, Train Loss: 0.004436,Eval Loss: 0.073834\n",
      "epoch: 212, Train Loss: 0.004314,Eval Loss: 0.073869\n",
      "epoch: 213, Train Loss: 0.004563,Eval Loss: 0.073825\n",
      "epoch: 214, Train Loss: 0.005168,Eval Loss: 0.073751\n",
      "epoch: 215, Train Loss: 0.005369,Eval Loss: 0.073723\n",
      "epoch: 216, Train Loss: 0.004255,Eval Loss: 0.073668\n",
      "epoch: 217, Train Loss: 0.004422,Eval Loss: 0.073630\n",
      "epoch: 218, Train Loss: 0.004657,Eval Loss: 0.073615\n",
      "epoch: 219, Train Loss: 0.004005,Eval Loss: 0.073605\n",
      "epoch: 220, Train Loss: 0.004471,Eval Loss: 0.073572\n",
      "epoch: 221, Train Loss: 0.004144,Eval Loss: 0.073573\n",
      "epoch: 222, Train Loss: 0.004089,Eval Loss: 0.073631\n",
      "epoch: 223, Train Loss: 0.004871,Eval Loss: 0.073666\n",
      "epoch: 224, Train Loss: 0.005433,Eval Loss: 0.073685\n",
      "epoch: 225, Train Loss: 0.004676,Eval Loss: 0.073752\n",
      "epoch: 226, Train Loss: 0.004936,Eval Loss: 0.073697\n",
      "epoch: 227, Train Loss: 0.004241,Eval Loss: 0.073652\n",
      "epoch: 228, Train Loss: 0.003825,Eval Loss: 0.073690\n",
      "epoch: 229, Train Loss: 0.005275,Eval Loss: 0.073695\n",
      "epoch: 230, Train Loss: 0.004470,Eval Loss: 0.073640\n",
      "epoch: 231, Train Loss: 0.005939,Eval Loss: 0.073590\n",
      "epoch: 232, Train Loss: 0.004348,Eval Loss: 0.073619\n",
      "epoch: 233, Train Loss: 0.005359,Eval Loss: 0.073681\n",
      "epoch: 234, Train Loss: 0.004152,Eval Loss: 0.073687\n",
      "epoch: 235, Train Loss: 0.004308,Eval Loss: 0.073670\n",
      "epoch: 236, Train Loss: 0.004779,Eval Loss: 0.073708\n",
      "epoch: 237, Train Loss: 0.004869,Eval Loss: 0.073751\n",
      "epoch: 238, Train Loss: 0.004092,Eval Loss: 0.073744\n",
      "epoch: 239, Train Loss: 0.005826,Eval Loss: 0.073709\n",
      "epoch: 240, Train Loss: 0.003851,Eval Loss: 0.073662\n",
      "epoch: 241, Train Loss: 0.003256,Eval Loss: 0.073628\n",
      "epoch: 242, Train Loss: 0.005830,Eval Loss: 0.073648\n",
      "epoch: 243, Train Loss: 0.005244,Eval Loss: 0.073646\n",
      "epoch: 244, Train Loss: 0.004174,Eval Loss: 0.073655\n",
      "epoch: 245, Train Loss: 0.003629,Eval Loss: 0.073697\n",
      "epoch: 246, Train Loss: 0.004023,Eval Loss: 0.073721\n",
      "epoch: 247, Train Loss: 0.004558,Eval Loss: 0.073744\n",
      "epoch: 248, Train Loss: 0.004298,Eval Loss: 0.073722\n",
      "epoch: 249, Train Loss: 0.004788,Eval Loss: 0.073698\n",
      "epoch: 250, Train Loss: 0.005134,Eval Loss: 0.073719\n",
      "epoch: 251, Train Loss: 0.004951,Eval Loss: 0.073708\n",
      "epoch: 252, Train Loss: 0.004142,Eval Loss: 0.073715\n",
      "epoch: 253, Train Loss: 0.005207,Eval Loss: 0.073716\n",
      "epoch: 254, Train Loss: 0.004098,Eval Loss: 0.073689\n",
      "epoch: 255, Train Loss: 0.003772,Eval Loss: 0.073709\n",
      "epoch: 256, Train Loss: 0.003845,Eval Loss: 0.073725\n",
      "epoch: 257, Train Loss: 0.004521,Eval Loss: 0.073767\n",
      "epoch: 258, Train Loss: 0.004769,Eval Loss: 0.073793\n",
      "epoch: 259, Train Loss: 0.003941,Eval Loss: 0.073797\n",
      "epoch: 260, Train Loss: 0.004746,Eval Loss: 0.073790\n",
      "epoch: 261, Train Loss: 0.004005,Eval Loss: 0.073720\n",
      "epoch: 262, Train Loss: 0.003503,Eval Loss: 0.073735\n",
      "epoch: 263, Train Loss: 0.004545,Eval Loss: 0.073776\n",
      "epoch: 264, Train Loss: 0.002944,Eval Loss: 0.073837\n",
      "epoch: 265, Train Loss: 0.003674,Eval Loss: 0.073862\n",
      "epoch: 266, Train Loss: 0.003765,Eval Loss: 0.073831\n",
      "epoch: 267, Train Loss: 0.004542,Eval Loss: 0.073811\n",
      "epoch: 268, Train Loss: 0.003922,Eval Loss: 0.073769\n",
      "epoch: 269, Train Loss: 0.003929,Eval Loss: 0.073801\n",
      "epoch: 270, Train Loss: 0.004424,Eval Loss: 0.073813\n",
      "epoch: 271, Train Loss: 0.005136,Eval Loss: 0.073811\n",
      "epoch: 272, Train Loss: 0.003482,Eval Loss: 0.073802\n",
      "epoch: 273, Train Loss: 0.004302,Eval Loss: 0.073779\n",
      "epoch: 274, Train Loss: 0.004410,Eval Loss: 0.073792\n",
      "epoch: 275, Train Loss: 0.004613,Eval Loss: 0.073808\n",
      "epoch: 276, Train Loss: 0.004658,Eval Loss: 0.073821\n",
      "epoch: 277, Train Loss: 0.004779,Eval Loss: 0.073769\n",
      "epoch: 278, Train Loss: 0.004741,Eval Loss: 0.073747\n",
      "epoch: 279, Train Loss: 0.005026,Eval Loss: 0.073750\n",
      "epoch: 280, Train Loss: 0.003532,Eval Loss: 0.073710\n",
      "epoch: 281, Train Loss: 0.004285,Eval Loss: 0.073660\n",
      "epoch: 282, Train Loss: 0.004687,Eval Loss: 0.073691\n",
      "epoch: 283, Train Loss: 0.006104,Eval Loss: 0.073688\n",
      "epoch: 284, Train Loss: 0.005045,Eval Loss: 0.073674\n",
      "epoch: 285, Train Loss: 0.004750,Eval Loss: 0.073660\n",
      "epoch: 286, Train Loss: 0.003754,Eval Loss: 0.073622\n",
      "epoch: 287, Train Loss: 0.005579,Eval Loss: 0.073687\n",
      "epoch: 288, Train Loss: 0.004274,Eval Loss: 0.073678\n",
      "epoch: 289, Train Loss: 0.004799,Eval Loss: 0.073660\n",
      "epoch: 290, Train Loss: 0.004717,Eval Loss: 0.073664\n",
      "epoch: 291, Train Loss: 0.004585,Eval Loss: 0.073696\n",
      "epoch: 292, Train Loss: 0.005138,Eval Loss: 0.073678\n",
      "epoch: 293, Train Loss: 0.005187,Eval Loss: 0.073670\n",
      "epoch: 294, Train Loss: 0.006175,Eval Loss: 0.073664\n",
      "epoch: 295, Train Loss: 0.005149,Eval Loss: 0.073626\n",
      "epoch: 296, Train Loss: 0.002781,Eval Loss: 0.073650\n",
      "epoch: 297, Train Loss: 0.003301,Eval Loss: 0.073728\n",
      "epoch: 298, Train Loss: 0.003836,Eval Loss: 0.073742\n",
      "epoch: 299, Train Loss: 0.002990,Eval Loss: 0.073748\n",
      "epoch: 300, Train Loss: 0.004417,Eval Loss: 0.073717\n",
      "epoch: 301, Train Loss: 0.003996,Eval Loss: 0.073695\n",
      "epoch: 302, Train Loss: 0.003817,Eval Loss: 0.073692\n",
      "epoch: 303, Train Loss: 0.003662,Eval Loss: 0.073715\n",
      "epoch: 304, Train Loss: 0.004706,Eval Loss: 0.073685\n",
      "epoch: 305, Train Loss: 0.004808,Eval Loss: 0.073613\n",
      "epoch: 306, Train Loss: 0.004808,Eval Loss: 0.073597\n",
      "epoch: 307, Train Loss: 0.004576,Eval Loss: 0.073611\n",
      "epoch: 308, Train Loss: 0.003944,Eval Loss: 0.073617\n",
      "epoch: 309, Train Loss: 0.004723,Eval Loss: 0.073601\n",
      "epoch: 310, Train Loss: 0.006027,Eval Loss: 0.073651\n",
      "epoch: 311, Train Loss: 0.004542,Eval Loss: 0.073699\n",
      "epoch: 312, Train Loss: 0.003209,Eval Loss: 0.073708\n",
      "epoch: 313, Train Loss: 0.004385,Eval Loss: 0.073734\n",
      "epoch: 314, Train Loss: 0.004385,Eval Loss: 0.073762\n",
      "epoch: 315, Train Loss: 0.003843,Eval Loss: 0.073708\n",
      "epoch: 316, Train Loss: 0.004228,Eval Loss: 0.073580\n",
      "epoch: 317, Train Loss: 0.004315,Eval Loss: 0.073538\n",
      "epoch: 318, Train Loss: 0.004571,Eval Loss: 0.073515\n",
      "epoch: 319, Train Loss: 0.005998,Eval Loss: 0.073538\n",
      "epoch: 320, Train Loss: 0.004409,Eval Loss: 0.073578\n",
      "epoch: 321, Train Loss: 0.005018,Eval Loss: 0.073567\n",
      "epoch: 322, Train Loss: 0.004293,Eval Loss: 0.073530\n",
      "epoch: 323, Train Loss: 0.004552,Eval Loss: 0.073470\n",
      "epoch: 324, Train Loss: 0.004433,Eval Loss: 0.073471\n",
      "epoch: 325, Train Loss: 0.004879,Eval Loss: 0.073449\n",
      "epoch: 326, Train Loss: 0.004201,Eval Loss: 0.073418\n",
      "epoch: 327, Train Loss: 0.003569,Eval Loss: 0.073451\n",
      "epoch: 328, Train Loss: 0.004389,Eval Loss: 0.073460\n",
      "epoch: 329, Train Loss: 0.003930,Eval Loss: 0.073497\n",
      "epoch: 330, Train Loss: 0.004322,Eval Loss: 0.073552\n",
      "epoch: 331, Train Loss: 0.004252,Eval Loss: 0.073523\n",
      "epoch: 332, Train Loss: 0.004112,Eval Loss: 0.073535\n",
      "epoch: 333, Train Loss: 0.004100,Eval Loss: 0.073570\n",
      "epoch: 334, Train Loss: 0.004278,Eval Loss: 0.073576\n",
      "epoch: 335, Train Loss: 0.004140,Eval Loss: 0.073596\n",
      "epoch: 336, Train Loss: 0.004005,Eval Loss: 0.073587\n",
      "epoch: 337, Train Loss: 0.005114,Eval Loss: 0.073582\n",
      "epoch: 338, Train Loss: 0.004009,Eval Loss: 0.073614\n",
      "epoch: 339, Train Loss: 0.003504,Eval Loss: 0.073662\n",
      "epoch: 340, Train Loss: 0.004953,Eval Loss: 0.073694\n",
      "epoch: 341, Train Loss: 0.004735,Eval Loss: 0.073725\n",
      "epoch: 342, Train Loss: 0.004806,Eval Loss: 0.073741\n",
      "epoch: 343, Train Loss: 0.002990,Eval Loss: 0.073727\n",
      "epoch: 344, Train Loss: 0.004193,Eval Loss: 0.073774\n",
      "epoch: 345, Train Loss: 0.004366,Eval Loss: 0.073822\n",
      "epoch: 346, Train Loss: 0.003447,Eval Loss: 0.073815\n",
      "epoch: 347, Train Loss: 0.003536,Eval Loss: 0.073822\n",
      "epoch: 348, Train Loss: 0.004410,Eval Loss: 0.073829\n",
      "epoch: 349, Train Loss: 0.003582,Eval Loss: 0.073853\n",
      "epoch: 350, Train Loss: 0.004497,Eval Loss: 0.073840\n",
      "epoch: 351, Train Loss: 0.004557,Eval Loss: 0.073837\n",
      "epoch: 352, Train Loss: 0.004802,Eval Loss: 0.073888\n",
      "epoch: 353, Train Loss: 0.004520,Eval Loss: 0.073895\n",
      "epoch: 354, Train Loss: 0.004694,Eval Loss: 0.073863\n",
      "epoch: 355, Train Loss: 0.003639,Eval Loss: 0.073856\n",
      "epoch: 356, Train Loss: 0.003541,Eval Loss: 0.073863\n",
      "epoch: 357, Train Loss: 0.005204,Eval Loss: 0.073834\n",
      "epoch: 358, Train Loss: 0.004589,Eval Loss: 0.073833\n",
      "epoch: 359, Train Loss: 0.004514,Eval Loss: 0.073843\n",
      "epoch: 360, Train Loss: 0.005454,Eval Loss: 0.073855\n",
      "epoch: 361, Train Loss: 0.003958,Eval Loss: 0.073885\n",
      "epoch: 362, Train Loss: 0.004949,Eval Loss: 0.073866\n",
      "epoch: 363, Train Loss: 0.004030,Eval Loss: 0.073809\n",
      "epoch: 364, Train Loss: 0.004346,Eval Loss: 0.073730\n",
      "epoch: 365, Train Loss: 0.003951,Eval Loss: 0.073711\n",
      "epoch: 366, Train Loss: 0.004024,Eval Loss: 0.073724\n",
      "epoch: 367, Train Loss: 0.004439,Eval Loss: 0.073722\n",
      "epoch: 368, Train Loss: 0.003980,Eval Loss: 0.073725\n",
      "epoch: 369, Train Loss: 0.004915,Eval Loss: 0.073698\n",
      "epoch: 370, Train Loss: 0.005590,Eval Loss: 0.073718\n",
      "epoch: 371, Train Loss: 0.004263,Eval Loss: 0.073744\n",
      "epoch: 372, Train Loss: 0.004009,Eval Loss: 0.073778\n",
      "epoch: 373, Train Loss: 0.004877,Eval Loss: 0.073808\n",
      "epoch: 374, Train Loss: 0.003942,Eval Loss: 0.073859\n",
      "epoch: 375, Train Loss: 0.004211,Eval Loss: 0.073920\n",
      "epoch: 376, Train Loss: 0.005337,Eval Loss: 0.073935\n",
      "epoch: 377, Train Loss: 0.004005,Eval Loss: 0.073931\n",
      "epoch: 378, Train Loss: 0.005418,Eval Loss: 0.073903\n",
      "epoch: 379, Train Loss: 0.003879,Eval Loss: 0.073940\n",
      "epoch: 380, Train Loss: 0.004233,Eval Loss: 0.073987\n",
      "epoch: 381, Train Loss: 0.004161,Eval Loss: 0.074002\n",
      "epoch: 382, Train Loss: 0.003904,Eval Loss: 0.074039\n",
      "epoch: 383, Train Loss: 0.004268,Eval Loss: 0.074076\n",
      "epoch: 384, Train Loss: 0.004326,Eval Loss: 0.074083\n",
      "epoch: 385, Train Loss: 0.004554,Eval Loss: 0.074064\n",
      "epoch: 386, Train Loss: 0.005353,Eval Loss: 0.074043\n",
      "epoch: 387, Train Loss: 0.003706,Eval Loss: 0.074003\n",
      "epoch: 388, Train Loss: 0.003920,Eval Loss: 0.073989\n",
      "epoch: 389, Train Loss: 0.005357,Eval Loss: 0.073984\n",
      "epoch: 390, Train Loss: 0.004189,Eval Loss: 0.073960\n",
      "epoch: 391, Train Loss: 0.005915,Eval Loss: 0.073900\n",
      "epoch: 392, Train Loss: 0.003926,Eval Loss: 0.073832\n",
      "epoch: 393, Train Loss: 0.004571,Eval Loss: 0.073802\n",
      "epoch: 394, Train Loss: 0.004330,Eval Loss: 0.073831\n",
      "epoch: 395, Train Loss: 0.005400,Eval Loss: 0.073819\n",
      "epoch: 396, Train Loss: 0.003491,Eval Loss: 0.073784\n",
      "epoch: 397, Train Loss: 0.004051,Eval Loss: 0.073749\n",
      "epoch: 398, Train Loss: 0.005008,Eval Loss: 0.073747\n",
      "epoch: 399, Train Loss: 0.004661,Eval Loss: 0.073732\n",
      "epoch: 400, Train Loss: 0.004910,Eval Loss: 0.073703\n",
      "epoch: 401, Train Loss: 0.003506,Eval Loss: 0.073620\n",
      "epoch: 402, Train Loss: 0.005130,Eval Loss: 0.073652\n",
      "epoch: 403, Train Loss: 0.003928,Eval Loss: 0.073642\n",
      "epoch: 404, Train Loss: 0.003956,Eval Loss: 0.073689\n",
      "epoch: 405, Train Loss: 0.004771,Eval Loss: 0.073710\n",
      "epoch: 406, Train Loss: 0.003917,Eval Loss: 0.073664\n",
      "epoch: 407, Train Loss: 0.003866,Eval Loss: 0.073701\n",
      "epoch: 408, Train Loss: 0.005709,Eval Loss: 0.073723\n",
      "epoch: 409, Train Loss: 0.005054,Eval Loss: 0.073714\n",
      "epoch: 410, Train Loss: 0.005293,Eval Loss: 0.073697\n",
      "epoch: 411, Train Loss: 0.003821,Eval Loss: 0.073662\n",
      "epoch: 412, Train Loss: 0.003282,Eval Loss: 0.073626\n",
      "epoch: 413, Train Loss: 0.005079,Eval Loss: 0.073599\n",
      "epoch: 414, Train Loss: 0.004616,Eval Loss: 0.073566\n",
      "epoch: 415, Train Loss: 0.004243,Eval Loss: 0.073528\n",
      "epoch: 416, Train Loss: 0.005126,Eval Loss: 0.073594\n",
      "epoch: 417, Train Loss: 0.003911,Eval Loss: 0.073612\n",
      "epoch: 418, Train Loss: 0.004710,Eval Loss: 0.073604\n",
      "epoch: 419, Train Loss: 0.004952,Eval Loss: 0.073559\n",
      "epoch: 420, Train Loss: 0.005366,Eval Loss: 0.073541\n",
      "epoch: 421, Train Loss: 0.004584,Eval Loss: 0.073598\n",
      "epoch: 422, Train Loss: 0.003494,Eval Loss: 0.073644\n",
      "epoch: 423, Train Loss: 0.004694,Eval Loss: 0.073649\n",
      "epoch: 424, Train Loss: 0.004849,Eval Loss: 0.073654\n",
      "epoch: 425, Train Loss: 0.005974,Eval Loss: 0.073678\n",
      "epoch: 426, Train Loss: 0.003913,Eval Loss: 0.073650\n",
      "epoch: 427, Train Loss: 0.003385,Eval Loss: 0.073638\n",
      "epoch: 428, Train Loss: 0.003832,Eval Loss: 0.073638\n",
      "epoch: 429, Train Loss: 0.004403,Eval Loss: 0.073629\n",
      "epoch: 430, Train Loss: 0.003158,Eval Loss: 0.073567\n",
      "epoch: 431, Train Loss: 0.006497,Eval Loss: 0.073541\n",
      "epoch: 432, Train Loss: 0.003887,Eval Loss: 0.073544\n",
      "epoch: 433, Train Loss: 0.003960,Eval Loss: 0.073489\n",
      "epoch: 434, Train Loss: 0.005044,Eval Loss: 0.073451\n",
      "epoch: 435, Train Loss: 0.004041,Eval Loss: 0.073424\n",
      "epoch: 436, Train Loss: 0.005136,Eval Loss: 0.073471\n",
      "epoch: 437, Train Loss: 0.004217,Eval Loss: 0.073463\n",
      "epoch: 438, Train Loss: 0.003921,Eval Loss: 0.073470\n",
      "epoch: 439, Train Loss: 0.003597,Eval Loss: 0.073467\n",
      "epoch: 440, Train Loss: 0.004345,Eval Loss: 0.073443\n",
      "epoch: 441, Train Loss: 0.005663,Eval Loss: 0.073439\n",
      "epoch: 442, Train Loss: 0.005356,Eval Loss: 0.073448\n",
      "epoch: 443, Train Loss: 0.004665,Eval Loss: 0.073453\n",
      "epoch: 444, Train Loss: 0.004116,Eval Loss: 0.073476\n",
      "epoch: 445, Train Loss: 0.004046,Eval Loss: 0.073493\n",
      "epoch: 446, Train Loss: 0.003772,Eval Loss: 0.073461\n",
      "epoch: 447, Train Loss: 0.003725,Eval Loss: 0.073385\n",
      "epoch: 448, Train Loss: 0.005069,Eval Loss: 0.073426\n",
      "epoch: 449, Train Loss: 0.004244,Eval Loss: 0.073490\n",
      "epoch: 450, Train Loss: 0.004496,Eval Loss: 0.073506\n",
      "epoch: 451, Train Loss: 0.004291,Eval Loss: 0.073491\n",
      "epoch: 452, Train Loss: 0.005517,Eval Loss: 0.073549\n",
      "epoch: 453, Train Loss: 0.004444,Eval Loss: 0.073633\n",
      "epoch: 454, Train Loss: 0.004464,Eval Loss: 0.073654\n",
      "epoch: 455, Train Loss: 0.003760,Eval Loss: 0.073657\n",
      "epoch: 456, Train Loss: 0.004467,Eval Loss: 0.073655\n",
      "epoch: 457, Train Loss: 0.005415,Eval Loss: 0.073629\n",
      "epoch: 458, Train Loss: 0.004851,Eval Loss: 0.073580\n",
      "epoch: 459, Train Loss: 0.004577,Eval Loss: 0.073582\n",
      "epoch: 460, Train Loss: 0.004080,Eval Loss: 0.073665\n",
      "epoch: 461, Train Loss: 0.004359,Eval Loss: 0.073650\n",
      "epoch: 462, Train Loss: 0.003556,Eval Loss: 0.073662\n",
      "epoch: 463, Train Loss: 0.003596,Eval Loss: 0.073662\n",
      "epoch: 464, Train Loss: 0.004633,Eval Loss: 0.073637\n",
      "epoch: 465, Train Loss: 0.004101,Eval Loss: 0.073608\n",
      "epoch: 466, Train Loss: 0.004186,Eval Loss: 0.073570\n",
      "epoch: 467, Train Loss: 0.008529,Eval Loss: 0.073590\n",
      "epoch: 468, Train Loss: 0.004203,Eval Loss: 0.073620\n",
      "epoch: 469, Train Loss: 0.004415,Eval Loss: 0.073642\n",
      "epoch: 470, Train Loss: 0.003738,Eval Loss: 0.073692\n",
      "epoch: 471, Train Loss: 0.004453,Eval Loss: 0.073702\n",
      "epoch: 472, Train Loss: 0.004753,Eval Loss: 0.073699\n",
      "epoch: 473, Train Loss: 0.004921,Eval Loss: 0.073660\n",
      "epoch: 474, Train Loss: 0.003442,Eval Loss: 0.073619\n",
      "epoch: 475, Train Loss: 0.003452,Eval Loss: 0.073630\n",
      "epoch: 476, Train Loss: 0.004831,Eval Loss: 0.073735\n",
      "epoch: 477, Train Loss: 0.004023,Eval Loss: 0.073799\n",
      "epoch: 478, Train Loss: 0.005569,Eval Loss: 0.073832\n",
      "epoch: 479, Train Loss: 0.004941,Eval Loss: 0.073824\n",
      "epoch: 480, Train Loss: 0.005446,Eval Loss: 0.073817\n",
      "epoch: 481, Train Loss: 0.004834,Eval Loss: 0.073876\n",
      "epoch: 482, Train Loss: 0.004064,Eval Loss: 0.073842\n",
      "epoch: 483, Train Loss: 0.004966,Eval Loss: 0.073805\n",
      "epoch: 484, Train Loss: 0.003960,Eval Loss: 0.073780\n",
      "epoch: 485, Train Loss: 0.004731,Eval Loss: 0.073789\n",
      "epoch: 486, Train Loss: 0.005594,Eval Loss: 0.073825\n",
      "epoch: 487, Train Loss: 0.005279,Eval Loss: 0.073807\n",
      "epoch: 488, Train Loss: 0.003835,Eval Loss: 0.073788\n",
      "epoch: 489, Train Loss: 0.004090,Eval Loss: 0.073737\n",
      "epoch: 490, Train Loss: 0.004971,Eval Loss: 0.073698\n",
      "epoch: 491, Train Loss: 0.005122,Eval Loss: 0.073668\n",
      "epoch: 492, Train Loss: 0.003564,Eval Loss: 0.073665\n",
      "epoch: 493, Train Loss: 0.004883,Eval Loss: 0.073674\n",
      "epoch: 494, Train Loss: 0.003600,Eval Loss: 0.073671\n",
      "epoch: 495, Train Loss: 0.004282,Eval Loss: 0.073652\n",
      "epoch: 496, Train Loss: 0.003636,Eval Loss: 0.073661\n",
      "epoch: 497, Train Loss: 0.005183,Eval Loss: 0.073625\n",
      "epoch: 498, Train Loss: 0.004619,Eval Loss: 0.073630\n",
      "epoch: 499, Train Loss: 0.003348,Eval Loss: 0.073666\n"
     ]
    }
   ],
   "source": [
    "#freeze，lr = 5e-4，2000 epoch\n",
    "#freeze，lr = 5e-5，2000 epoch \n",
    "#unfreeze，lr = 1e-6 or smaller，2000 epoch\n",
    "\n",
    "#unfreeze all layers\n",
    "for param in finetune_net.parameters():\n",
    "\tparam.requires_grad = True\n",
    "\n",
    "\n",
    "#lr = 5e-5\n",
    "lr = 1e-6\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, finetune_net.parameters()), lr)\n",
    "#optimizer = torch.optim.Adam(finetune_net.parameters(), lr)\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.1)\n",
    "\n",
    "train_losses = []\n",
    "train_acces = []\n",
    "\n",
    "eval_losses = []\n",
    "eval_acces = []\n",
    "\n",
    "\n",
    "for e in range(500):\n",
    "\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    finetune_net.train()   #\n",
    "\n",
    "    for features, labels in dataLoader_train:\n",
    "\n",
    "        # forward, get loss\n",
    "        outputs = finetune_net(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backpropagate\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_losses.append(train_loss / myDataset_train.len)\n",
    "\n",
    "    #test on validation set\n",
    "    eval_loss = 0\n",
    "    eval_acc = 0\n",
    "    finetune_net.eval() \n",
    "\n",
    "\n",
    "    for features, labels in dataLoader_valid:\n",
    "        features = Variable(features)  \n",
    "        labels = Variable(labels)  \n",
    "\n",
    "        outputs = finetune_net(features)  \n",
    "        # label = label.unsqueeze(1)\n",
    "        loss = criterion(outputs, labels) \n",
    "\n",
    "        # record loss\n",
    "        eval_loss += loss.item()\n",
    "        \n",
    "#         pred = outputs.max(1)[1]\n",
    "#         num_correct = (pred==labels).sum().item()\n",
    "#         acc = num_correct / x.shape[0]\n",
    "#         eval_acc += acc\n",
    "\n",
    "    eval_losses.append(eval_loss / myDataset_valid.len)\n",
    "    # eval_acces.append(eval_acc / myDataset_test.len)\n",
    "    print('epoch: {}, Train Loss: {:.6f},Eval Loss: {:.6f}'\n",
    "          .format(e, train_loss / myDataset_train.len, eval_loss / myDataset_valid.len))\n",
    "    #log_writer.add_scalar(\"loss/train\", float(train_loss), e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7a2dd96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([64, 1000]),\n",
       " torch.Size([64]),\n",
       " torch.Size([1, 64]),\n",
       " torch.Size([1])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[para.shape for para in finetune_net.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf0f448c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.Net"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(finetune_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2560249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = pd.read_csv('test_features.csv')\n",
    "test_id = test_X.Id.values\n",
    "test_X = test_X.drop(columns=['Id', 'smiles']).values\n",
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98ac314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_net.eval()\n",
    "result = finetune_net(torch.Tensor(test_X)).detach().numpy()\n",
    "result = pd.DataFrame(result, index=test_id).reset_index(drop=False).rename({'index': 'Id', 0:'y'}, axis=1).to_csv('finetunewed.txt', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becbf5e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c7b4bb862106ac383dfbdd3d2c03390c19d090ce3f74ce128fbff848985c161"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('intro-ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
